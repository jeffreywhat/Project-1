---
title: "Intro to DS - KNN Classifiers: PIMA dataset"
author: "GWU Intro to Data Science DATS 6101"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r include=FALSE}
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
library(MASS)
library(ggplot2)
library(class)
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# HW Assignment - KNN

This exercise uses the Pima.te and Pima.tr dataset from the MASS package, which includes measurement on a population of women of Pima Indian heritage living near Phoenix, Arizona. The women were tested for diabetes according to World Health Organization criteria.  We are interested in predicting diabetes status as a function of the other variables.

The variables in the dataset are:

* `npreg` : number of pregnancies.
* `glu` : plasma glucose concentration
* `bp` : diastolic blood pressure (mm Hg) skin triceps skinfold thickness (mm)
* `bmi` : body mass index
* `ped` : diabetes pedigree function
* `age` : age in years
* `type` : Yes or No: diabetic by WHO criteria  


## Pima Dataset  

### Question 1  
**Obtain the dataset**  
In the `MASS` library, combine the two datasets `Pima.te` and `Pima.tr` back into one complete dataset, call it `pima`. (Try function `rbind()`.) How many observations are there?  

```{r}
pima <- rbind(Pima.te, Pima.tr)
nrow(pima)
```

There are 532 observations.

### Question 2  
**Summary**  
Obtain some basic summary statistics for the full dataset `pima`.

```{r, results='markup'}
str(pima)
summary(pima)
```

### Question 3  
**Pairs**  
Another quick EDA to perform, you can plot the `pairs()`. The plot function can handle both numerical and categorical variable type. After trying the function in the R base library, also try the modified version with `pairs.panels()`. Comment on interesting relationships.  


```{r, eval=FALSE}
## Example of pairs.panel on the iris data set -- edit for your analysis of the pima data ##
loadPkg("psych")
pairs.panels(iris[,-5], 
             method = "pearson", # correlation method
             hist.col = "#00AFBB", # set histogram color, can use "#22AFBB", "red",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
```

```{r, results='markup'}
loadPkg("psych")
pairs(pima)
pairs.panels(pima)
unloadPkg("psych")
```

There is high correlation (.5 or greater) between `glu` and `type`, `skin` and `bmi`, and `age` and `npreg`. Other than that, some of the spreads seem to have a pattern no matter what they're plotted against. `glu` doesn't seem to have significant correlation with anything other than type, and skin/bmi have a clump of observations. 

## KNN  

### Question 4  
**Train-Test split 3:1**  
First, standardize all of the X variables (i.e. all by `type`).  Now, in order to perform KNN analysis, we need to separate the X variables and the y variable `type`.  Before we separate them out, create a vector/array of 1s and 2s to create a train-test split in the ratio of 3:1 (i.e. 75% training, 25% test). Make sure to set a constant seed value (e.g. using `set.seed`) so that you can duplicate the results.  You should end up with a dataframe of Xs for training with the corresponding vector of y, and a dataframe of Xs for testing with the corresponding vector of y.  Make sure the order of observations in train-X and train-y are not mixed up during the process. Same for test-X and test-y.  

```{r, results='markup'}
scaledpima <- as.data.frame(scale(pima[1:7], center = TRUE, scale = TRUE))
set.seed(2)
pima_sample <- sample(2, nrow(scaledpima), replace=TRUE, prob=c(0.75, 0.25))

pima_sample

train_X <- scaledpima[pima_sample==1, 1:7]
test_X <- scaledpima[pima_sample==2, 1:7]
train_y <- pima[pima_sample==1, 8]
test_y <- pima[pima_sample==2, 8]
```

### Question 5  
**KNN results**  
Perform the KNN analysis, with different k values. You do not need to show all the results from different k, but please include the one with the best (total) accuracy in your submission. How does the accuracy compared to the percentages of being T/F in the dataset?  

```{r, eval=FALSE, results='hide'}
loadPkg("FNN")
loadPkg("gmodels")
loadPkg("caret")

ResultDf = data.frame(k= numeric(0), Total.Accuracy = numeric(0), row.names = NULL)

for (kval in 3:20) {
  pima_pred <- knn(train = train_X, test = test_X, cl=train_y, k=kval)
  pima_pred_cross <- CrossTable(test_y, pima_pred, prop.chisq = FALSE)
  print(paste("k = ", kval))
  
  cm = confusionMatrix(pima_pred, reference = test_y) # from caret library

  cmaccu = cm$overall['Accuracy']
  print(paste("Total Accuracy = ", cmaccu))

  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL) 
  
  ResultDf = rbind(ResultDf, cmt)
  print(xkabledply(as.matrix(cm), title = paste("ConfusionMatrix for k = ", kval)))
  print(xkabledply(data.frame(cm$byClass), title = paste("k = ", kval)))
}
```

```{r, results='markup'}
loadPkg("FNN")
loadPkg("gmodels")
loadPkg("caret")
pima_pred <- knn(train = train_X, test = test_X, cl=train_y, k=20, prob = TRUE)
pima_pred_cross <- CrossTable(test_y, pima_pred, prop.chisq = FALSE)
cm = confusionMatrix(pima_pred, reference = test_y)
cmaccu = cm$overall['Accuracy']
print(paste("Total Accuracy = ", cmaccu))

yes_count = sum(pima$type=='Yes')
no_count = sum(pima$type=='No')
no_count/(yes_count+no_count)
unloadPkg("caret")
unloadPkg("gmodels")
```

The accuracy for the final model chosen was for K of 20, with an accuracy of about .831. This is a decent improvement over the proportion of type Nos over the total, which was about .667. However, the model predicted most of the real Nos correctly, 96/99, but only predicted 22/43 Yes's correctly. That means that this model is about 50% on the real Yes's, which could definitely be a problem of too many false negatives when predicting pregnancies, since in cases like this, it could be possibly better to predict a false postive than a false negative.

## Logistic Regression and comparison  

### Question 6   
**Logistic Regression results**  
Compare to the best logistic regression you can get. (Use the full model with all variables, since that is what we have for KNN.) How is the accuracy (assumes the standard cutoff of 0.5) compared to KNN?  

```{r, results="markup"}
library(ModelMetrics)
library(pROC)
library(ResourceSelection)
library(pscl)
attach(pima)
pimaLogit <- glm(type ~ npreg + glu + bp + bmi + ped + age, family = "binomial")
summary(pimaLogit)
xkabledply(confusionMatrix(actual=pimaLogit$y, predicted=pimaLogit$fitted.values), title = "Confusion Matrix")
pimaNullLogit <- glm(type ~ 1, family = "binomial")
mcFadden = 1 - logLik(pimaLogit)/logLik(pimaNullLogit)
mcFadden
pR2(pimaLogit)
```

The overall accuracy of the logistic regression, with all of the data as a the sample, is lower, at an accuracy of .793. Notably, however, the logistic regression has a much lower false negative rate, which could be important in this context.

### Question 7  
**ROC-AUC**  
What is the AUC for the logit model?  Plot the ROC.  We should be able to compute the ROC and AUC for the KNN model the same way. Can you compare them?   

```{r, results="markup"}
prob = predict(pimaLogit, type = "response")
pima$prob = prob
h <- roc(type~prob)
auc(h)
plot(h)

attributes(pima_pred)$prob
h <- roc(test_y~attributes(pima_pred)$prob)
auc(h)
plot(h)
```

The AUC of the logit model is .861, which is over .8 and probably considered not bad. On the other hand, the AUC of the KNN model is only .79, and a higher specificity trades a lot of sensitivity, which makes some sense because our model was relatively quite bad at predicting the Yes's (due to the fact that we were looking to maximize accuracy, not sensitivity and specificity). Still, there is the concern of over fitting with the logit model, as it is a full model which could possibly lead to too many insignificant covariates.


